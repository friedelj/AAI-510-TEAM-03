{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa2VZo9HIfxVwKg59+iY5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/AAI-510-TEAM-03/blob/main/IOT_Team2_CNN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Team 2 CNN Rework to reduce over fitting"
      ],
      "metadata": {
        "id": "eIW6nSZCO0Y6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOm8mJ52OuLX"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "WjL0AuvBPKaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "QOYAgceTPPKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "FPTwVW3RPTfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "def check_images(directory):\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img.verify()  # Verify image integrity\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted image found: {file_path}\")\n",
        "                os.remove(file_path)  # Optionally remove the corrupted file\n",
        "\n",
        "check_images(path)"
      ],
      "metadata": {
        "id": "aEg0VERSPaiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(\"path_to_your_dataset\")\n",
        "valid_extensions = ['.jpg', '.jpeg', '.png']\n",
        "\n",
        "for file_path in data_dir.glob(\"*/*\"):\n",
        "    if file_path.suffix.lower() not in valid_extensions:\n",
        "        print(f\"Removing non-image file: {file_path}\")\n",
        "        file_path.unlink()  # Delete the non-image file"
      ],
      "metadata": {
        "id": "6Moe0zOZPgkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_images(directory):\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img = img.convert(\"RGB\")  # Convert to RGB\n",
        "                img.save(file_path)  # Overwrite with corrected image\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping problematic image: {file_path}\")\n",
        "\n",
        "convert_images(path)"
      ],
      "metadata": {
        "id": "jSpob15VPjcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetClassifier:\n",
        "\n",
        "    def __init__(self, data_dir='PetImages', img_size=224, batch_size=32):\n",
        "        self.data_dir = pathlib.Path(data_dir)\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.setup_hardware()\n",
        "\n",
        "    def setup_hardware(self):\n",
        "        \"\"\"Configure TensorFlow for available hardware.\"\"\"\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "                print(f\"Training on {len(gpus)} GPU(s) with mixed precision\")\n",
        "            except RuntimeError as e:\n",
        "                print(f\"GPU setup error: {e}\")\n",
        "                print(\"Training on CPU\")\n",
        "        else:\n",
        "            print(\"No GPU found. Training on CPU\")\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        \"\"\"Create TensorFlow dataset from directory structure.\"\"\"\n",
        "        # Create dataset\n",
        "        data = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.data_dir,\n",
        "            validation_split=0.2,\n",
        "            subset=\"training\",\n",
        "            seed=123,\n",
        "            image_size=(self.img_size, self.img_size),\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.data_dir,\n",
        "            validation_split=0.2,\n",
        "            subset=\"validation\",\n",
        "            seed=123,\n",
        "            image_size=(self.img_size, self.img_size),\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        # Configure dataset for performance\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "        data = data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "        val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        return data, val_data\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create the CNN model.\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            # Input layer\n",
        "            tf.keras.layers.Input(shape=(self.img_size, self.img_size, 3)),\n",
        "\n",
        "            # First conv block\n",
        "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "            # Second conv block\n",
        "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "            # Third conv block\n",
        "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "            # Dense layers\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # L2 regularization\n",
        "            tf.keras.layers.Dropout(0.6),  # Increased dropout\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=20):  # Increased epochs for better training\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        try:\n",
        "            # Prepare data\n",
        "            train_ds, val_ds = self.prepare_dataset()\n",
        "\n",
        "            # Apply data augmentation\n",
        "            data_augmentation = tf.keras.Sequential([\n",
        "                tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "                tf.keras.layers.RandomRotation(0.2),\n",
        "                tf.keras.layers.RandomZoom(0.2),\n",
        "            ])\n",
        "\n",
        "            # Apply augmentation to the training dataset using map\n",
        "            train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
        "\n",
        "            # Build and compile model\n",
        "            model = self.build_model()\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            # Callbacks\n",
        "            callbacks = [\n",
        "                tf.keras.callbacks.ModelCheckpoint(\n",
        "                    'best_model.keras',  # Change from .h5 to .keras\n",
        "                    save_best_only=True,\n",
        "                    monitor='val_accuracy'\n",
        "                ),\n",
        "                tf.keras.callbacks.EarlyStopping(\n",
        "                    monitor='val_accuracy',\n",
        "                    patience=5,\n",
        "                    restore_best_weights=True\n",
        "                ),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                    monitor='val_loss',\n",
        "                    factor=0.5,\n",
        "                    patience=3\n",
        "                ),\n",
        "                tf.keras.callbacks.TensorBoard(\n",
        "                    log_dir='./logs',\n",
        "                    histogram_freq=1\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            # Train the model\n",
        "            history = model.fit(\n",
        "                train_ds,\n",
        "                validation_data=val_ds,\n",
        "                epochs=epochs,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "            return model, history  # Ensure the return statement is reached\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training: {e}\")\n",
        "            return None, None"
      ],
      "metadata": {
        "id": "D3G1NoYAP0UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifier with downloaded dataset path\n",
        "classifier = PetClassifier(data_dir=path)\n",
        "\n",
        "# Train the model\n",
        "model, history = classifier.train(epochs=20)"
      ],
      "metadata": {
        "id": "iBMiRmoVP9Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'], label='Training Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "0eQPWvoiQQru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('pet_classifier_model.keras')\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "U7mvKdgFQUYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}