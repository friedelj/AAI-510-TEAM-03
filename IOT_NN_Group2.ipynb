{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNriy1DrPthdJpotoyhobxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/AAI-510-TEAM-03/blob/main/IOT_NN_Group2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbeBuG0_Dw6E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub tensorflow"
      ],
      "metadata": {
        "id": "XvNq-pcQD4Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "Nhr8RdbGD4Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "s4mQKLIbD4F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "1ah6wTY2D4CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "def check_images(directory):\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img.verify()  # Verify image integrity\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted image found: {file_path}\")\n",
        "                os.remove(file_path)  # Optionally remove the corrupted file\n",
        "\n",
        "check_images(path)"
      ],
      "metadata": {
        "id": "1yc4IC6SD3-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(\"path_to_your_dataset\")\n",
        "valid_extensions = ['.jpg', '.jpeg', '.png']\n",
        "\n",
        "for file_path in data_dir.glob(\"*/*\"):\n",
        "    if file_path.suffix.lower() not in valid_extensions:\n",
        "        print(f\"Removing non-image file: {file_path}\")\n",
        "        file_path.unlink()  # Delete the non-image file"
      ],
      "metadata": {
        "id": "kqltGKl8D37L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_images(directory):\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                img = Image.open(file_path)\n",
        "                img = img.convert(\"RGB\")  # Convert to RGB\n",
        "                img.save(file_path)  # Overwrite with corrected image\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping problematic image: {file_path}\")\n",
        "\n",
        "convert_images(path)"
      ],
      "metadata": {
        "id": "SaPxcjhGD339"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PetClassifier:\n",
        "    def __init__(self, data_dir='PetImages', img_size=224, batch_size=32):\n",
        "        self.data_dir = pathlib.Path(data_dir)\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.setup_hardware()\n",
        "\n",
        "    def setup_hardware(self):\n",
        "        \"\"\"Configure TensorFlow for available hardware.\"\"\"\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "                print(f\"Training on {len(gpus)} GPU(s) with mixed precision\")\n",
        "            except RuntimeError as e:\n",
        "                print(f\"GPU setup error: {e}\")\n",
        "                print(\"Training on CPU\")\n",
        "        else:\n",
        "            print(\"No GPU found. Training on CPU\")\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        \"\"\"Create TensorFlow dataset from directory structure.\"\"\"\n",
        "        # Create dataset\n",
        "        data = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.data_dir,\n",
        "            validation_split=0.2,\n",
        "            subset=\"training\",\n",
        "            seed=123,\n",
        "            image_size=(self.img_size, self.img_size),\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.data_dir,\n",
        "            validation_split=0.2,\n",
        "            subset=\"validation\",\n",
        "            seed=123,\n",
        "            image_size=(self.img_size, self.img_size),\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        # Configure dataset for performance\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "        data = data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "        val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        return data, val_data\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create the CNN model.\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            # Input layer\n",
        "            tf.keras.layers.Input(shape=(self.img_size, self.img_size, 3)),\n",
        "\n",
        "            # First conv block\n",
        "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "            # Second conv block\n",
        "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "            # Third conv block\n",
        "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "            # Dense layers\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        # Prepare data\n",
        "        train_ds, val_ds = self.prepare_dataset()\n",
        "\n",
        "        # Build and compile model\n",
        "        model = self.build_model()\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                #'best_model.h5',\n",
        "                'best_model.keras',  # Change from .h5 to .keras\n",
        "                save_best_only=True,\n",
        "                monitor='val_accuracy'\n",
        "            ),\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=3\n",
        "            ),\n",
        "            tf.keras.callbacks.TensorBoard(\n",
        "                log_dir='./logs',\n",
        "                histogram_freq=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        return model, history"
      ],
      "metadata": {
        "id": "nGMbDKVmD30L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifier with downloaded dataset path\n",
        "classifier = PetClassifier(data_dir=path)\n",
        "\n",
        "# Train the model\n",
        "model, history = classifier.train(epochs=10)"
      ],
      "metadata": {
        "id": "VB6Uc3AOD3wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'], label='Training Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "9zCjoFZNEsEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('pet_classifier_model.keras')\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "p-MirglyEsB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKCBWb9AEr_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import os\n",
        "\n",
        "class PetAutoencoder:\n",
        "    def __init__(self, data_dir='PetImages', img_size=224, batch_size=32):\n",
        "        self.data_dir = pathlib.Path(data_dir)\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.setup_hardware()\n",
        "\n",
        "    def setup_hardware(self):\n",
        "        \"\"\"Configure TensorFlow for available hardware.\"\"\"\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "                print(f\"Training on {len(gpus)} GPU(s) with mixed precision\")\n",
        "            except RuntimeError as e:\n",
        "                print(f\"GPU setup error: {e}\")\n",
        "                print(\"Training on CPU\")\n",
        "        else:\n",
        "            print(\"No GPU found. Training on CPU\")\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        \"\"\"Create TensorFlow dataset from directory structure.\"\"\"\n",
        "        dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.data_dir,\n",
        "            validation_split=0.2,\n",
        "            subset=\"training\",\n",
        "            seed=123,\n",
        "            image_size=(self.img_size, self.img_size),\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.data_dir,\n",
        "            validation_split=0.2,\n",
        "            subset=\"validation\",\n",
        "            seed=123,\n",
        "            image_size=(self.img_size, self.img_size),\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        # Normalize images to [0,1] range\n",
        "        def normalize_img(image, label):\n",
        "            return image / 255.0, image  # Target is the image itself\n",
        "\n",
        "        dataset = dataset.map(normalize_img)\n",
        "        val_dataset = val_dataset.map(normalize_img)\n",
        "\n",
        "        AUTOTUNE = tf.data.AUTOTUNE\n",
        "        dataset = dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "        val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "        return dataset, val_dataset\n",
        "\n",
        "    def build_autoencoder(self):\n",
        "        \"\"\"Build the Autoencoder model.\"\"\"\n",
        "\n",
        "        # Encoder\n",
        "        encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(self.img_size, self.img_size, 3)),\n",
        "            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.MaxPooling2D(),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(256, activation='relu', name=\"latent_space\")  # Latent space representation\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense((self.img_size // 8) * (self.img_size // 8) * 128, activation='relu'),\n",
        "            tf.keras.layers.Reshape(((self.img_size // 8), (self.img_size // 8), 128)),\n",
        "            tf.keras.layers.Conv2DTranspose(128, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.UpSampling2D(),\n",
        "            tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.UpSampling2D(),\n",
        "            tf.keras.layers.Conv2DTranspose(32, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.UpSampling2D(),\n",
        "            tf.keras.layers.Conv2DTranspose(3, 3, activation='sigmoid', padding='same')  # Reconstruct image\n",
        "        ])\n",
        "\n",
        "        # Full Autoencoder\n",
        "        autoencoder = tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
        "\n",
        "        return autoencoder\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        \"\"\"Train the autoencoder.\"\"\"\n",
        "        train_ds, val_ds = self.prepare_dataset()\n",
        "\n",
        "        model = self.build_autoencoder()\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='mse',  # Mean Squared Error for reconstruction\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                'best_autoencoder.keras',\n",
        "                save_best_only=True,\n",
        "                monitor='val_loss'\n",
        "            ),\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir='./logs_autoencoder', histogram_freq=1)\n",
        "        ]\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        return model, history"
      ],
      "metadata": {
        "id": "d1iH9MQQEr77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize classifier with downloaded dataset path\n",
        "classifier = PetAutoencoder(data_dir=path)\n",
        "\n",
        "# Train the model\n",
        "model, history = classifier.train(epochs=3)"
      ],
      "metadata": {
        "id": "0nJUVmxBEr45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'], label='Training Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "mZmZ1Fo1Er1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCvpjfNCErzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-RbpkXnErvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8IHPOsVoErqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLbgwUuUErgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJbCJp31D3sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06n1G8huD3ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7w-oXZ2CD3Gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}