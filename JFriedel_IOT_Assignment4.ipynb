{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP9GqfYT6HXfQWCSgejYKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/AAI-510-TEAM-03/blob/main/JFriedel_IOT_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joseph Friedel                          AAI 530                     Assignment 4                             3 February 2025"
      ],
      "metadata": {
        "id": "ZJBtPfs_jfoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short Term Memory Networks for IoT Prediction"
      ],
      "metadata": {
        "id": "IyXAvsm_jomi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNs and LSTM models are very popular neural network architectures when working with sequential data, since they both carry some \"memory\" of previous\n",
        "inputs when predicting the next output. In this assignment we will continue to work with the Household Electricity Consumption dataset and use an LSTM\n",
        "model to predict the Global Active Power (GAP) from a sequence of previous GAP readings. You will build one model following the directions in this\n",
        "notebook closely, then you will be asked to make changes to that original model and analyze the effects that they had on the model performance. You will\n",
        "also be asked to compare the performance of your LSTM model to the linear regression predictor that you built in last week's assignment."
      ],
      "metadata": {
        "id": "Evp12-m0js-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Assignment Instructions"
      ],
      "metadata": {
        "id": "lIUunABVjxty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading\n",
        "it.\n",
        "\n",
        "One sign of mature code is conforming to a style guide. We recommend the Google Python Style Guide. If you use a different style guide, please include a\n",
        "cell with a link.\n",
        "\n",
        "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final\n",
        "submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential import statements and make sure\n",
        "that all such statements are moved into the designated cell.\n",
        "\n",
        "When you save your notebook as a pdf, make sure that all cell output is visible (even error messages) as this will aid your instructor in grading your\n",
        "work.\n",
        "\n",
        "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions\n",
        "to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. Make sure to answer every question marked with a\n",
        "Q: for full credit."
      ],
      "metadata": {
        "id": "roWsN7wyj1Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, LSTM, Activation\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "x4c433_ukE5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use this cell to import additional libraries or define helper functions\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "wWvo1yT7kJ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare your data"
      ],
      "metadata": {
        "id": "JGHbU7QKkPwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll once again be using the cleaned household electricity consumption data from the previous two assignments. I recommend saving your dataset by\n",
        "running df.to_csv(\"filename\") at the end of assignment 2 so that you don't have to re-do your cleaning steps. If you are not confident in your own\n",
        "cleaning steps, you may ask your instructor for a cleaned version of the data. You will not be graded on the cleaning steps in this assignment, but\n",
        "some functions may not work if you use the raw data.\n",
        "\n",
        "Unlike when using Linear Regression to make our predictions for Global Active Power (GAP), LSTM requires that we have a pre-trained model when our\n",
        "predictive software is shipped (the ability to iterate on the model after it's put into production is another question for another day). Thus, we will\n",
        "train the model on a segment of our data and then measure its performance on simulated streaming data another segment of the data. Our dataset is very\n",
        "large, so for speed's sake, we will limit ourselves to 1% of the entire dataset."
      ],
      "metadata": {
        "id": "R6j8kz0akUDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Import your data, select the a random 1% of the dataset, and then split it 80/20 into training and validation sets (the test split will come from\n",
        "the training data as part of the tensorflow LSTM model call). HINT: Think carefully about how you do your train/validation split--does it make sense to\n",
        "randomize the data?"
      ],
      "metadata": {
        "id": "xn4m5j1qkYQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last few rows of the DataFrame to confirm it loaded correctly\n",
        "print(df.tail())"
      ],
      "metadata": {
        "id": "UU6q_S5akeFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "JoWTjTRDkimU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a copy of df before reducing it\n",
        "df_clean = df.copy()"
      ],
      "metadata": {
        "id": "9YhVXdaDkmNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create your training and validation sets here\n",
        "#assign size for data subset\n",
        "num_rows = int(len(df) * 0.01)\n",
        "\n",
        "#take random data subset\n",
        "df = df.sample(n=num_rows, random_state=42).sort_index().copy()"
      ],
      "metadata": {
        "id": "NytPf2vRkr7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "7MNT_qUDkuJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data subset 80/20 for train/validation\n",
        "# Determine the number of rows for training and validation\n",
        "num_rows = len(df)\n",
        "num_val = int(num_rows * 0.2)  # 20% for validation\n",
        "num_train = num_rows - num_val  # Remaining 80% for training\n",
        "\n",
        "# Randomly sample validation indices, then sort them sequentially\n",
        "val_indices = df.sample(n=num_val, random_state=42).index\n",
        "val_indices = sorted(val_indices)  # Ensure sequential order\n",
        "\n",
        "# Create validation and training DataFrames\n",
        "val_df = df.loc[val_indices].copy()\n",
        "train_df = df.drop(val_indices).copy()"
      ],
      "metadata": {
        "id": "edDbsk6HkxY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset the indices for cleanliness\n",
        "train_df = train_df.reset_index()\n",
        "val_df = val_df.reset_index()"
      ],
      "metadata": {
        "id": "hUYTouYZk19D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "ZOLgYOdqk58z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to create our input and output sequences. In the lab session this week, we used an LSTM model to make a binary prediction, but LSTM models\n",
        "are very flexible in what they can output: we can also use them to predict a single real-numbered output (we can even use them to predict a sequence of\n",
        "outputs). Here we will train a model to predict a single real-numbered output such that we can compare our model directly to the linear regression model\n",
        "from last week."
      ],
      "metadata": {
        "id": "AFg_duCvk941"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Create a nested list structure for the training data, with a sequence of GAP measurements as the input and the GAP measurement at your predictive\n",
        "horizon as your expected output"
      ],
      "metadata": {
        "id": "ydu6PRM5lCM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_arrays = []\n",
        "seq_labs = []"
      ],
      "metadata": {
        "id": "o0WNLwjDlFvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll start out with a 30 minute input sequence and a 5 minute predictive horizon\n",
        "# we don't need to work in seconds this time, since we'll just use the indices instead of a unix timestamp\n",
        "seq_length = 30\n",
        "ph = 5\n",
        "\n",
        "feat_cols = ['Global_active_power']"
      ],
      "metadata": {
        "id": "9_-yJm9UlK9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create list of sequence length GAP readings\n",
        "for i in range(len(train_df) - seq_length - ph):\n",
        "    # Extract input sequence (last `seq_length` readings)\n",
        "    seq_arrays.append(train_df[feat_cols].iloc[i : i + seq_length].values)\n",
        "\n",
        "    # Extract expected output (GAP measurement at `ph` steps ahead)\n",
        "    seq_labs.append(train_df['Global_active_power'].iloc[i + seq_length + ph])"
      ],
      "metadata": {
        "id": "A_AbgQ31lQKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to numpy arrays and floats to appease keras/tensorflow\n",
        "seq_arrays = np.array(seq_arrays, dtype = object).astype(np.float32)\n",
        "seq_labs = np.array(seq_labs, dtype = object).astype(np.float32)"
      ],
      "metadata": {
        "id": "e7keuYIElT6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assertions to ensure correct shape\n",
        "assert seq_arrays.shape == (len(train_df) - seq_length - ph, seq_length, len(feat_cols))\n",
        "assert seq_labs.shape == (len(train_df) - seq_length - ph,)"
      ],
      "metadata": {
        "id": "8s_cBG8LlYVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shape of input sequences\n",
        "seq_arrays.shape"
      ],
      "metadata": {
        "id": "v7HY6PaXlcc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: What is the function of the assert statements in the above cell? Why do we use assertions in our code?"
      ],
      "metadata": {
        "id": "S22KMDwDlgHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: The assert statements are used for debugging and validation. They make sure that the seq_arrays and seq_labs variables have the needed shape before\n",
        "being used by the LSTM model.  It is used to find errors early in the program.  If seq_arrays and seq_labs have the wrong shapes, the assertions will\n",
        "raise an error early on.  It ensures the correct data structures for LSTM:  seq_arrays is expected to be a 3D NumPy array and seq_labs is expected\n",
        "to be a 1D NumPy array.  If the shapes do not match the expected format an Assertion Error is raised."
      ],
      "metadata": {
        "id": "NfGG4s1-ljbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "916dq9zglm49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will begin with a model architecture very similar to the model we built in the lab session. We will have two LSTM layers, with 5 and 3 hidden units\n",
        "respectively, and we will apply dropout after each LSTM layer. However, we will use a LINEAR final layer and MSE for our loss function, since our output\n",
        "is continuous instead of binary."
      ],
      "metadata": {
        "id": "47KpYGz9lp9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Fill in all values marked with a ?? in the cell below"
      ],
      "metadata": {
        "id": "KgL0LVsdltPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to save model\n",
        "model_path = 'LSTM_model1.keras'\n",
        "\n",
        "# Build the network\n",
        "# Define the number of input features and output dimensions\n",
        "nb_features = len(feat_cols)  # Number of features (should be 1 for GAP readings)\n",
        "nb_out = 1  # Single real-valued output (next GAP reading at predictive horizon)\n",
        "\n",
        "# Build the LSTM network\n",
        "model = Sequential()\n",
        "\n",
        "# Add first LSTM layer\n",
        "model.add(LSTM(\n",
        "    input_shape=(seq_length, nb_features),  # (time steps, features)\n",
        "    units=50,  # Number of LSTM neurons\n",
        "    return_sequences=True))  # Return sequences for stacking layers\n",
        "model.add(Dropout(0.2))  # Prevent overfitting\n",
        "\n",
        "# Add second LSTM layer\n",
        "model.add(LSTM(\n",
        "    units=50,  # Number of neurons in second layer\n",
        "    return_sequences=False))  # Final LSTM layer does not return sequences\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Add fully connected layer for output\n",
        "model.add(Dense(units=nb_out))  # Predict a single value\n",
        "model.add(Activation('linear'))  # Use a linear activation function for regression\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate = 0.01)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer,metrics=['mse'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(seq_arrays, seq_labs,  # Training data (X, y)\n",
        "                    epochs=100,\n",
        "                    batch_size=500,\n",
        "                    validation_split=0.05,  # Use 5% of training data for validation\n",
        "                    verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "          )\n",
        "\n",
        "# List all data in history\n",
        "print(history.history.keys())"
      ],
      "metadata": {
        "id": "dQRcxDzklwu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the code from the book to visualize our training progress and model performance"
      ],
      "metadata": {
        "id": "jWs5-DKHl4ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for Loss/MSE\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss/MSE')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"LSTM_loss1.png\")"
      ],
      "metadata": {
        "id": "mDoeGpESl8Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validating our model"
      ],
      "metadata": {
        "id": "j5GceghumAyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create our simulated streaming validation set to test our model \"in production\". With our linear regression models, we were able to\n",
        "begin making predictions with only two datapoints, but the LSTM model requires an input sequence of seq_length to make a prediction. We can get around\n",
        "this limitation by \"padding\" our inputs when they are too short."
      ],
      "metadata": {
        "id": "AG8BtTJ-mEjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: create a nested list structure for the validation data, with a sequence of GAP measurements as the input and the GAP measurement at your\n",
        "predictive horizon as your expected output. Begin your predictions after only two GAP measurements are available, and check out this keras function to\n",
        "automatically pad sequences that are too short."
      ],
      "metadata": {
        "id": "Lb5_QSKSmI0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Describe the pad_sequences function and how it manages sequences of variable length. What does the \"padding\" argument determine, and which setting\n",
        "makes the most sense for our use case here?"
      ],
      "metadata": {
        "id": "9AD9Ox3nmMtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: Format:\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_sequences = pad_sequences(sequences,\n",
        "                                 maxlen=None,\n",
        "                                 dtype='int32',\n",
        "                                 padding='pre',\n",
        "                                 truncating='pre',\n",
        "                                 value=0.0)\n",
        "\n",
        "Argument \"sequences\" is a List of lists, with each sublist being a sequence of vary length.\n",
        "Argument \"maxlen\" is the maximum sequence length, which results in longer sequences being truncated, while shorter sequences become padded.\n",
        "The \"dtype\" is the Data type, like float32.\n",
        "Variable \"padding\" tells where to add padding: 'pre' is before and 'post' is after.\n",
        "Variable \"truncating\" tells\twhere to truncate longer sequences: 'pre' is at the beginning and 'post' is at the end.\n",
        "The variable \"value\" is the value used for padding, and defaults to 0.0.\n",
        "\n",
        "When using \"pad_sequences\" short sequences are padded to reach maxlen.  Long sequences are shortened if they exceed maxlen.  This ensures all\n",
        "    sequeces have the same length for deep learning model batch processing.\n",
        "\n",
        "I'll use padding = 'pre' because I'm working with time-series forcasting, using LSTM.   LSTM uses \"past to present\" sequence processing, so using\n",
        "    padding in the beqinning makes sure that most recent data is at the end of the sequence.  The most recent data is the most useful for prediction."
      ],
      "metadata": {
        "id": "jstdsDBPmQCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_arrays = []\n",
        "val_labs = []\n",
        "\n",
        "# Create list of GAP readings starting with a minimum of two readings\n",
        "for i in range(2, len(val_df) - ph):  # Start from index 2 to ensure at least two readings\n",
        "    # Extract input sequence (from the start to current index)\n",
        "    val_arrays.append(val_df[feat_cols].iloc[:i].values)\n",
        "\n",
        "    # Extract expected output (GAP measurement at predictive horizon)\n",
        "    val_labs.append(val_df['Global_active_power'].iloc[i + ph])\n",
        "\n",
        "# Use the pad_sequences function on your input sequences\n",
        "# remember that we will later want our datatype to be np.float32\n",
        "val_arrays = pad_sequences(val_arrays, maxlen=seq_length, padding='pre', dtype='float32')\n",
        "\n",
        "# Convert labels to numpy arrays and floats to appease keras/tensorflow\n",
        "val_labs = np.array(val_labs, dtype = object).astype(np.float32)"
      ],
      "metadata": {
        "id": "U6rUB-H3mU9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now run this validation data through our LSTM model and visualize its performance like we did on the linear regression data."
      ],
      "metadata": {
        "id": "t5uVMWJlmYsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on validation data\n",
        "predictions = model.predict(val_arrays)\n",
        "\n",
        "# Create a time axis for plotting\n",
        "time_axis = range(len(val_labs))  # X-axis representing time\n",
        "\n",
        "# Plot actual vs. predicted values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(time_axis, val_labs, label=\"Actual Values\", color='blue', linestyle='solid')\n",
        "plt.plot(time_axis, predictions, label=\"Predicted Values\", color='red', linestyle='dashed')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Time Steps\")\n",
        "plt.ylabel(\"Global Active Power (GAP)\")\n",
        "plt.title(\"LSTM Predictions vs. Actual GAP Values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VY7uoZG9mcgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
        "print('\\nMSE: {}'.format(scores_test[1]))\n",
        "\n",
        "y_pred_test = model.predict(val_arrays)\n",
        "y_true_test = val_labs\n",
        "\n",
        "test_set = pd.DataFrame(y_pred_test)\n",
        "test_set.to_csv('submit_test.csv', index = None)\n",
        "\n",
        "# Plot the predicted data vs. the actual data\n",
        "# We will limit our plot to the first 500 predictions for better visualization\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test[-500:], label = 'Predicted Value')\n",
        "plt.plot(y_true_test[-500:], label = 'Actual Value')\n",
        "plt.title('Global Active Power Prediction - Last 500 Points', fontsize=22, fontweight='bold')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('row')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_regression_verify.png\")"
      ],
      "metadata": {
        "id": "THU-1xQCmiyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: How did your model perform? What can you tell about the model from the loss curves? What could we do to try to improve the model?"
      ],
      "metadata": {
        "id": "LcUE_i0Qmn8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: The model did fair:  it landed within the data range, and matched the real data in upward and downward slopes, but the model\n",
        "did not match the range of the actual data.  The model ranged from about o.5 to about 1.5 while the actual data ranged from about 0.\n",
        "to 6.   From the loss curves, I could be underfitting, because the model is too simple or I could have a too high learning rate, causing the\n",
        "optimizer to miss important patterns or I could have too little training, so the model hasn't learned trends well enough.  So I could increase\n",
        "the units in the LSTM layers or add more LSTM layers, or reduce the dropout, if underfitting is the problem.  Or I could reduce the learnin rate, or\n",
        "change the optimizer if the learning rate is the problem.  If training is the issue, I can increase epochs."
      ],
      "metadata": {
        "id": "JJBjxiy0mrnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Optimization"
      ],
      "metadata": {
        "id": "PKRLkCF1mvvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's your turn to build an LSTM-based model in hopes of improving performance on this training set. Changes that you might consider include:\n",
        "\n",
        "Add more variables to the input sequences\n",
        "Change the optimizer and/or adjust the learning rate\n",
        "Change the sequence length and/or the predictive horizon\n",
        "Change the number of hidden layers in each of the LSTM layers\n",
        "Change the model architecture altogether--think about adding convolutional layers, linear layers, additional regularization, creating embeddings for the\n",
        "input data, changing the loss function, etc."
      ],
      "metadata": {
        "id": "BAAOd7HAm2LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There isn't any minimum performance increase or number of changes that need to be made, but I want to see that you have tried some different things.\n",
        "Remember that building and optimizing deep learning networks is an art and can be very difficult, so don't make yourself crazy trying to optimize for\n",
        "this assignment."
      ],
      "metadata": {
        "id": "YxM9PO5em6hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: What changes are you going to try with your model? Why do you think these changes could improve model performance?"
      ],
      "metadata": {
        "id": "vRvkQ5Cdm9_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well I'll try lowering the Learning Rate to try to catch finer patterns. Then I'll try increasing the LSTM complexity,\n",
        "like adding layers, in case my model is too simple.  Then I'll ensure the data is shaped correctly, In case I'm inputting\n",
        "the data incorrectly, and that's causing the poor results."
      ],
      "metadata": {
        "id": "YoXtgU-snBad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# play with your ideas for optimization here"
      ],
      "metadata": {
        "id": "oZoKgyAhnFy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# show me how one or two of your different models perform\n",
        "# using the code from the \"Validating our model\" section above"
      ],
      "metadata": {
        "id": "lQVjRACAnJuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Lowering the learning rate"
      ],
      "metadata": {
        "id": "SW_95ZfVnOnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])"
      ],
      "metadata": {
        "id": "pKRQpp6xnSFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vi7Um0cqnVrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit(seq_arrays, seq_labs,  # Training data (X, y)\n",
        "                    epochs=100,\n",
        "                    batch_size=500,\n",
        "                    validation_split=0.05,  # Use 5% of training data for validation\n",
        "                    verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "          )"
      ],
      "metadata": {
        "id": "jlrpf61WnXz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_arrays = []\n",
        "val_labs = []\n",
        "\n",
        "# Create list of GAP readings starting with a minimum of two readings\n",
        "for i in range(2, len(val_df) - ph):  # Start from index 2 to ensure at least two readings\n",
        "    # Extract input sequence (from the start to current index)\n",
        "    val_arrays.append(val_df[feat_cols].iloc[:i].values)\n",
        "\n",
        "    # Extract expected output (GAP measurement at predictive horizon)\n",
        "    val_labs.append(val_df['Global_active_power'].iloc[i + ph])\n",
        "\n",
        "# Use the pad_sequences function on your input sequences\n",
        "# remember that we will later want our datatype to be np.float32\n",
        "val_arrays = pad_sequences(val_arrays, maxlen=seq_length, padding='pre', dtype='float32')\n",
        "\n",
        "# Convert labels to numpy arrays and floats to appease keras/tensorflow\n",
        "val_labs = np.array(val_labs, dtype = object).astype(np.float32)"
      ],
      "metadata": {
        "id": "oPHgYezenbQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
        "print('\\nMSE: {}'.format(scores_test[1]))\n",
        "\n",
        "y_pred_test = model.predict(val_arrays)\n",
        "y_true_test = val_labs\n",
        "\n",
        "test_set = pd.DataFrame(y_pred_test)\n",
        "test_set.to_csv('submit_test.csv', index = None)\n",
        "\n",
        "# Plot the predicted data vs. the actual data\n",
        "# We will limit our plot to the first 500 predictions for better visualization\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test[-500:], label = 'Predicted Value')\n",
        "plt.plot(y_true_test[-500:], label = 'Actual Value')\n",
        "plt.title('Global Active Power Prediction - Last 500 Points', fontsize=22, fontweight='bold')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('row')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_regression_verify.png\")"
      ],
      "metadata": {
        "id": "zgcv_6mbnfcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No change :("
      ],
      "metadata": {
        "id": "W_LZtJhVnkr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Increasing LSTM complexity:"
      ],
      "metadata": {
        "id": "4ll7wSgZnots"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the input shape of your data is (samples, timesteps, features)\n",
        "# Make sure seq_arrays is shaped (samples, seq_length, nb_features)\n",
        "seq_arrays = np.reshape(seq_arrays, (seq_arrays.shape[0], seq_arrays.shape[1], nb_features))\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first LSTM layer\n",
        "model.add(LSTM(\n",
        "    units=100,              # Increase neurons to capture more complexity\n",
        "    return_sequences=True,  # Keep sequences for the next LSTM layer\n",
        "    input_shape=(seq_length, nb_features)))  # Correct input shape (timesteps, features)\n",
        "model.add(Dropout(0.1))  # Reduce dropout slightly\n",
        "\n",
        "# Add the second LSTM layer\n",
        "model.add(LSTM(\n",
        "    units=100,  # Increase neurons in the second LSTM layer\n",
        "    return_sequences=False))  # Don't return sequences from the last LSTM\n",
        "model.add(Dropout(0.1))  # Reduce dropout\n",
        "\n",
        "# Add Dense layer for output\n",
        "model.add(Dense(units=nb_out))  # Output a single prediction (next GAP value)\n",
        "model.add(Activation('linear'))  # Linear activation for regression output\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Lower learning rate\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model with training data\n",
        "history = model.fit(seq_arrays, seq_labs,  # Training data\n",
        "                    epochs=200,  # Increase the number of epochs\n",
        "                    batch_size=500,\n",
        "                    validation_split=0.05,  # 5% of training data used for validation\n",
        "                    verbose=2,\n",
        "                    callbacks=[\n",
        "                        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='min'),\n",
        "                        keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=0)\n",
        "                    ])"
      ],
      "metadata": {
        "id": "cBrBnJ3MnsXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_arrays = []\n",
        "val_labs = []\n",
        "\n",
        "# Create list of GAP readings starting with a minimum of two readings\n",
        "for i in range(2, len(val_df) - ph):  # Start from index 2 to ensure at least two readings\n",
        "    # Extract input sequence (from the start to current index)\n",
        "    val_arrays.append(val_df[feat_cols].iloc[:i].values)\n",
        "\n",
        "    # Extract expected output (GAP measurement at predictive horizon)\n",
        "    val_labs.append(val_df['Global_active_power'].iloc[i + ph])\n",
        "\n",
        "# Use the pad_sequences function on your input sequences\n",
        "# remember that we will later want our datatype to be np.float32\n",
        "val_arrays = pad_sequences(val_arrays, maxlen=seq_length, padding='pre', dtype='float32')\n",
        "\n",
        "# Convert labels to numpy arrays and floats to appease keras/tensorflow\n",
        "val_labs = np.array(val_labs, dtype = object).astype(np.float32)"
      ],
      "metadata": {
        "id": "HTbdTRgPnyVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
        "print('\\nMSE: {}'.format(scores_test[1]))\n",
        "\n",
        "y_pred_test = model.predict(val_arrays)\n",
        "y_true_test = val_labs\n",
        "\n",
        "test_set = pd.DataFrame(y_pred_test)\n",
        "test_set.to_csv('submit_test.csv', index = None)\n",
        "\n",
        "# Plot the predicted data vs. the actual data\n",
        "# We will limit our plot to the first 500 predictions for better visualization\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test[-500:], label = 'Predicted Value')\n",
        "plt.plot(y_true_test[-500:], label = 'Actual Value')\n",
        "plt.title('Global Active Power Prediction - Last 500 Points', fontsize=22, fontweight='bold')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('row')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_regression_verify.png\")"
      ],
      "metadata": {
        "id": "PHTG3U3ln2Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No change :("
      ],
      "metadata": {
        "id": "uqHB6BF6n7tM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Reshaping Data"
      ],
      "metadata": {
        "id": "2OwLJp6jn_KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the input shape of data is (samples, timesteps, features)\n",
        "# Make sure seq_arrays is shaped (samples, seq_length, nb_features)\n",
        "seq_arrays = np.reshape(seq_arrays, (seq_arrays.shape[0], seq_arrays.shape[1], nb_features))\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first LSTM layer\n",
        "model.add(LSTM(\n",
        "    units=100,              # Increase neurons to capture more complexity\n",
        "    return_sequences=True,  # Keep sequences for the next LSTM layer\n",
        "    input_shape=(seq_length, nb_features)))  # Correct input shape (timesteps, features)\n",
        "model.add(Dropout(0.1))  # Reduce dropout slightly\n",
        "\n",
        "# Add the second LSTM layer\n",
        "model.add(LSTM(\n",
        "    units=100,  # Increase neurons in the second LSTM layer\n",
        "    return_sequences=False))  # Don't return sequences from the last LSTM\n",
        "model.add(Dropout(0.1))  # Reduce dropout\n",
        "\n",
        "# Add Dense layer for output\n",
        "model.add(Dense(units=nb_out))  # Output a single prediction (next GAP value)\n",
        "model.add(Activation('linear'))  # Linear activation for regression output\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Lower learning rate\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Fit the model with training data\n",
        "history = model.fit(seq_arrays, seq_labs,  # Training data\n",
        "                    epochs=200,  # Increase the number of epochs\n",
        "                    batch_size=500,\n",
        "                    validation_split=0.05,  # 5% of training data used for validation\n",
        "                    verbose=2,\n",
        "                    callbacks=[\n",
        "                        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='min'),\n",
        "                        keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=0)\n",
        "                    ])"
      ],
      "metadata": {
        "id": "u1lfbJ67oCP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_arrays = []\n",
        "val_labs = []\n",
        "\n",
        "# Create list of GAP readings starting with a minimum of two readings\n",
        "for i in range(2, len(val_df) - ph):  # Start from index 2 to ensure at least two readings\n",
        "    # Extract input sequence (from the start to current index)\n",
        "    val_arrays.append(val_df[feat_cols].iloc[:i].values)\n",
        "\n",
        "    # Extract expected output (GAP measurement at predictive horizon)\n",
        "    val_labs.append(val_df['Global_active_power'].iloc[i + ph])\n",
        "\n",
        "# Use the pad_sequences function on your input sequences\n",
        "# remember that we will later want our datatype to be np.float32\n",
        "val_arrays = pad_sequences(val_arrays, maxlen=seq_length, padding='pre', dtype='float32')\n",
        "\n",
        "# Convert labels to numpy arrays and floats to appease keras/tensorflow\n",
        "val_labs = np.array(val_labs, dtype = object).astype(np.float32)"
      ],
      "metadata": {
        "id": "3d1n_g8GoIaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
        "print('\\nMSE: {}'.format(scores_test[1]))\n",
        "\n",
        "y_pred_test = model.predict(val_arrays)\n",
        "y_true_test = val_labs\n",
        "\n",
        "test_set = pd.DataFrame(y_pred_test)\n",
        "test_set.to_csv('submit_test.csv', index = None)\n",
        "\n",
        "# Plot the predicted data vs. the actual data\n",
        "# We will limit our plot to the first 500 predictions for better visualization\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test[-500:], label = 'Predicted Value')\n",
        "plt.plot(y_true_test[-500:], label = 'Actual Value')\n",
        "plt.title('Global Active Power Prediction - Last 500 Points', fontsize=22, fontweight='bold')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('row')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_regression_verify.png\")"
      ],
      "metadata": {
        "id": "NdV_tZYroL2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No change :("
      ],
      "metadata": {
        "id": "UpciirhwoQ7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: How did your model changes affect performance on the validation data? Why do you think they were/were not effective? If you were trying to optimize\n",
        "    for production, what would you try next?"
      ],
      "metadata": {
        "id": "SwT4HHVSoUh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: No change!  That makes me suspect there is not a problem with the model.  So that makes me suspect the data.  The next change I would do is take a\n",
        "larger sampling of data: instead of 1%, maybe 5%."
      ],
      "metadata": {
        "id": "mHDpX5HnoXYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: How did the models that you built in this assignment compare to the linear regression model from last week? Think about model performance and other\n",
        "IoT device considerations; Which model would you choose to use in an IoT system that predicts GAP for a single household with a 5-minute predictive\n",
        "horizon, and why?"
      ],
      "metadata": {
        "id": "UToPqYfAoaaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: I thought the Linear Regression prediction followed the data beter and the MSE was lower.  For a 5-minute horizon, I would use\n",
        "the LSTM model.  I think LSTM is more capable to showing the highly varying, complex  patterns of GAP than LR.  LR would be better for modeling\n",
        "data with less variance over time."
      ],
      "metadata": {
        "id": "bz1QtB8goe5u"
      }
    }
  ]
}