{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAXFm1ql51ol2yd4f7a3w5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/friedelj/AAI-510-TEAM-03/blob/main/JFriedel_IoT_Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JFriedel                  IOT                 Assignment 6          18 Feb. 2025"
      ],
      "metadata": {
        "id": "kRFh9btZ7Xr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT generated code , generated 18 February 2025"
      ],
      "metadata": {
        "id": "jDHjiaNQ90Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "shapes = [\"circle\", \"square\", \"triangle\"]\n",
        "\n",
        "# Function to generate a random color\n",
        "def random_color():\n",
        "    return tuple(random.randint(0, 255) for _ in range(3))\n",
        "\n",
        "# Function to draw a circle\n",
        "def draw_circle(draw, size):\n",
        "    x0 = random.randint(0, size - 50)\n",
        "    y0 = random.randint(0, size - 50)\n",
        "    diameter = random.randint(20, min(size - x0, size - y0))\n",
        "    draw.ellipse([x0, y0, x0 + diameter, y0 + diameter], fill=random_color())\n",
        "\n",
        "# Function to draw a square\n",
        "def draw_square(draw, size):\n",
        "    x0 = random.randint(0, size - 50)\n",
        "    y0 = random.randint(0, size - 50)\n",
        "    side = random.randint(20, min(size - x0, size - y0))\n",
        "    draw.rectangle([x0, y0, x0 + side, y0 + side], fill=random_color())\n",
        "\n",
        "# Function to draw a triangle\n",
        "def draw_triangle(draw, size):\n",
        "    x1 = random.randint(0, size)\n",
        "    y1 = random.randint(0, size)\n",
        "    x2 = random.randint(0, size)\n",
        "    y2 = random.randint(0, size)\n",
        "    x3 = random.randint(0, size)\n",
        "    y3 = random.randint(0, size)\n",
        "    draw.polygon([x1, y1, x2, y2, x3, y3], fill=random_color())\n",
        "\n",
        "# Function to generate images\n",
        "def generate_images(shape, count, size=256):\n",
        "    shape_dir = os.path.join(data_dir, shape)\n",
        "    if not os.path.exists(shape_dir):\n",
        "        os.makedirs(shape_dir)\n",
        "\n",
        "    for i in range(count):\n",
        "        image = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        if shape == \"circle\":\n",
        "            draw_circle(draw, size)\n",
        "        elif shape == \"square\":\n",
        "            draw_square(draw, size)\n",
        "        elif shape == \"triangle\":\n",
        "            draw_triangle(draw, size)\n",
        "\n",
        "        image_path = os.path.join(shape_dir, f\"{shape}_{i + 1}.png\")\n",
        "        image.save(image_path)\n",
        "\n",
        "# Generate 100 images for each shape\n",
        "for shape in shapes:\n",
        "    generate_images(shape, 100)\n",
        "\n",
        "print(\"Image generation complete!\")"
      ],
      "metadata": {
        "id": "WoU-DiO-72Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGQU-IDK7WCz"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "# Function to randomly select and transfer images to validation directory\n",
        "# Create validation directory if it doesn't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "def transfer_validation_images(shape, count):\n",
        "    shape_dir = os.path.join(data_dir, shape)\n",
        "    files = [f for f in os.listdir(shape_dir) if os.path.isfile(os.path.join(shape_dir, f))]\n",
        "    selected_files = random.sample(files, count)\n",
        "\n",
        "    for file in selected_files:\n",
        "        src_path = os.path.join(shape_dir, file)\n",
        "        dst_path = os.path.join(validation_dir, file)\n",
        "        shutil.move(src_path, dst_path)\n",
        "for shape in shapes:\n",
        "    transfer_validation_images(shape, 10)\n",
        "print(\"validation transfer complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class ShapeClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShapeClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ShapeClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "class_correct = [0] * 3\n",
        "class_total = [0] * 3\n",
        "with torch.no_grad():\n",
        "    for images, labels in validation_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Accuracy of {shapes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
        "\n",
        "print(\"Validation complete!\")"
      ],
      "metadata": {
        "id": "ABbTlQXY74zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Create validation directory and class subdirectories if they don't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "    for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "        os.makedirs(os.path.join(validation_dir, shape), exist_ok=True)\n",
        "\n",
        "shapes = [\"circle\", \"square\", \"triangle\"]\n",
        "\n",
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class ShapeClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShapeClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ShapeClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "class_correct = [0] * 3\n",
        "class_total = [0] * 3\n",
        "with torch.no_grad():\n",
        "    for images, labels in validation_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Accuracy of {shapes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
        "\n",
        "print(\"Validation complete!\")"
      ],
      "metadata": {
        "id": "BOckPcmS74l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Create validation directory and class subdirectories if they don't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_dir = os.path.join(validation_dir, shape)\n",
        "    if not os.path.exists(shape_dir):\n",
        "        os.makedirs(shape_dir)\n",
        "\n",
        "shapes = [\"circle\", \"square\", \"triangle\"]\n",
        "\n",
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class ShapeClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShapeClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ShapeClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "class_correct = [0] * 3\n",
        "class_total = [0] * 3\n",
        "with torch.no_grad():\n",
        "    for images, labels in validation_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Accuracy of {shapes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
        "\n",
        "print(\"Validation complete!\")"
      ],
      "metadata": {
        "id": "_3ywy-5X8HbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Create validation directory and class subdirectories if they don't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_dir = os.path.join(validation_dir, shape)\n",
        "    if not os.path.exists(shape_dir):\n",
        "        os.makedirs(shape_dir)\n",
        "\n",
        "# Automatically copy images to validation subdirectories\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_data_dir = os.path.join(data_dir, shape)\n",
        "    shape_validation_dir = os.path.join(validation_dir, shape)\n",
        "    if os.path.exists(shape_data_dir):\n",
        "        images = [f for f in os.listdir(shape_data_dir) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "        selected_images = random.sample(images, min(10, len(images)))\n",
        "        for image in selected_images:\n",
        "            shutil.copy(os.path.join(shape_data_dir, image), os.path.join(shape_validation_dir, image))\n",
        "\n",
        "shapes = [\"circle\", \"square\", \"triangle\"]\n",
        "\n",
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class ShapeClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ShapeClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ShapeClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Evaluate the model on validation data\n",
        "class_correct = [0] * 3\n",
        "class_total = [0] * 3\n",
        "with torch.no_grad():\n",
        "    for images, labels in validation_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Accuracy of {shapes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\")\n",
        "\n",
        "print(\"Validation complete!\")"
      ],
      "metadata": {
        "id": "roKSvhna8sCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Create validation directory and class subdirectories if they don't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_dir = os.path.join(validation_dir, shape)\n",
        "    if not os.path.exists(shape_dir):\n",
        "        os.makedirs(shape_dir)\n",
        "\n",
        "# Automatically copy images to validation subdirectories\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_data_dir = os.path.join(data_dir, shape)\n",
        "    shape_validation_dir = os.path.join(validation_dir, shape)\n",
        "    if os.path.exists(shape_data_dir):\n",
        "        images = [f for f in os.listdir(shape_data_dir) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "        selected_images = random.sample(images, min(10, len(images)))\n",
        "        for image in selected_images:\n",
        "            shutil.copy(os.path.join(shape_data_dir, image), os.path.join(shape_validation_dir, image))\n",
        "\n",
        "# Load datasets\n",
        "batch_size = 32\n",
        "img_size = (256, 256)\n",
        "\n",
        "train_dataset = image_dataset_from_directory(data_dir, shuffle=True, batch_size=batch_size, image_size=img_size)\n",
        "validation_dataset = image_dataset_from_directory(validation_dir, shuffle=True, batch_size=batch_size, image_size=img_size)\n",
        "\n",
        "# Define the CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(3)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs)\n",
        "\n",
        "# Evaluate on validation data\n",
        "val_loss, val_acc = model.evaluate(validation_dataset)\n",
        "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
        "\n",
        "# Accuracy per class\n",
        "class_names = train_dataset.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "class_correct = {name: 0 for name in class_names}\n",
        "class_total = {name: 0 for name in class_names}\n",
        "\n",
        "for images, labels in validation_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    for i in range(len(labels)):\n",
        "        label = class_names[labels[i].numpy()]\n",
        "        if predicted_labels[i] == labels[i]:\n",
        "            class_correct[label] += 1\n",
        "        class_total[label] += 1\n",
        "\n",
        "for shape in class_names:\n",
        "    accuracy = 100 * class_correct[shape] / class_total[shape]\n",
        "    print(f\"Accuracy of {shape}: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"Validation complete!\")"
      ],
      "metadata": {
        "id": "GhI64QZL8r-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Create validation directory and class subdirectories if they don't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_dir = os.path.join(validation_dir, shape)\n",
        "    if not os.path.exists(shape_dir):\n",
        "        os.makedirs(shape_dir)\n",
        "\n",
        "# Automatically copy images to validation subdirectories\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_data_dir = os.path.join(data_dir, shape)\n",
        "    shape_validation_dir = os.path.join(validation_dir, shape)\n",
        "    if os.path.exists(shape_data_dir):\n",
        "        images = [f for f in os.listdir(shape_data_dir) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "        selected_images = random.sample(images, min(10, len(images)))\n",
        "        for image in selected_images:\n",
        "            shutil.copy(os.path.join(shape_data_dir, image), os.path.join(shape_validation_dir, image))\n",
        "\n",
        "# Load datasets\n",
        "batch_size = 32\n",
        "img_size = (256, 256)\n",
        "\n",
        "train_dataset = image_dataset_from_directory(data_dir, shuffle=True, batch_size=batch_size, image_size=img_size)\n",
        "validation_dataset = image_dataset_from_directory(validation_dir, shuffle=True, batch_size=batch_size, image_size=img_size)\n",
        "\n",
        "# Define the CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(3)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs)\n",
        "\n",
        "# Evaluate on validation data\n",
        "val_loss, val_acc = model.evaluate(validation_dataset)\n",
        "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
        "\n",
        "# Accuracy per class\n",
        "class_names = train_dataset.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "class_correct = {name: 0 for name in class_names}\n",
        "class_total = {name: 0 for name in class_names}\n",
        "\n",
        "for images, labels in validation_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    for i in range(len(labels)):\n",
        "        label = class_names[labels[i].numpy()]\n",
        "        if predicted_labels[i] == labels[i]:\n",
        "            class_correct[label] += 1\n",
        "        class_total[label] += 1\n",
        "\n",
        "for shape in class_names:\n",
        "    accuracy = 100 * class_correct[shape] / class_total[shape]\n",
        "    print(f\"Accuracy of {shape}: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"Validation complete!\")"
      ],
      "metadata": {
        "id": "P0bh25Z38r7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set up directories\n",
        "base_dir = r\"C:/Users/josep\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "validation_dir = os.path.join(base_dir, \"validation data\")\n",
        "\n",
        "# Create validation directory and class subdirectories if they don't exist\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.makedirs(validation_dir)\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_dir = os.path.join(validation_dir, shape)\n",
        "    if not os.path.exists(shape_dir):\n",
        "        os.makedirs(shape_dir)\n",
        "\n",
        "# Automatically copy images to validation subdirectories\n",
        "for shape in [\"circle\", \"square\", \"triangle\"]:\n",
        "    shape_data_dir = os.path.join(data_dir, shape)\n",
        "    shape_validation_dir = os.path.join(validation_dir, shape)\n",
        "    if os.path.exists(shape_data_dir):\n",
        "        images = [f for f in os.listdir(shape_data_dir) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
        "        selected_images = random.sample(images, min(10, len(images)))\n",
        "        for image in selected_images:\n",
        "            shutil.copy(os.path.join(shape_data_dir, image), os.path.join(shape_validation_dir, image))\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Convert grayscale to RGB\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "validation_dataset = datasets.ImageFolder(root=validation_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the Autoencoder model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = Autoencoder().cuda()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the Autoencoder\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for data in train_loader:\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'autoencoder.pth')\n",
        "\n",
        "# To use the encoder for classification:\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, autoencoder):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = autoencoder.encoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "encoder = Encoder(model).cuda()\n",
        "# You can now use 'encoder' to extract features and train a classifier on top of these features."
      ],
      "metadata": {
        "id": "lf5IsTAq8r4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2Y2KvSB8r0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIoaMYch8rth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9d4aS5a8rjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRBVp91u8HSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddmi8mir8HIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}